{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports - Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar dados dos ficheiros auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Assuming the dataset is in CSV format and contains the features and the target\n",
    "df = pd.read_csv('heart-disease.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1) \n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stratified 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define classifiers\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "knn_accuracies = cross_val_score(knn, X, y, cv=cv, scoring='accuracy')\n",
    "nb_accuracies = cross_val_score(nb, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Boxplots\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=[knn_accuracies, nb_accuracies], palette=\"Set2\")\n",
    "plt.xticks([0, 1], ['KNN (k=5)', 'Naive Bayes'])\n",
    "plt.title('KNN vs Naive Bayes Cross-Validation Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Print fold accuracies\n",
    "print(f\"KNN accuracies: {knn_accuracies}\")\n",
    "print(f\"Naive Bayes accuracies: {nb_accuracies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes (GNB) shows a higher average accuracy compared to k-Nearest Neighbors (kNN) with ùëò = 5.\n",
    "\n",
    "GNB appears more stable as the interquartile range is much narrower compared to kNN, which indicates less variability in the accuracy across different folds.\n",
    "\n",
    "On the other hand, kNN shows more variability, with a wider box and longer whiskers, suggesting that its performance fluctuates more across the cross-validation folds.\n",
    "\n",
    "The Naive Bayes classifier is not only more accurate but also more stable than kNN in this scenario. The higher stability could be attributed to the probabilistic nature of Naive Bayes, which is less affected by small variations in the data compared to kNN, which relies on distances and local neighborhoods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5-fold Stratified Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Scale the data using Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Assuming X is your feature matrix\n",
    "\n",
    "# Initialize the models\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Cross-validate and store accuracies\n",
    "knn_scaled_scores = cross_val_score(knn, X_scaled, y, cv=cv)\n",
    "nb_scaled_scores = cross_val_score(nb, X_scaled, y, cv=cv)\n",
    "\n",
    "# Create boxplot for accuracies across folds (Scaled Models)\n",
    "data = [knn_scaled_scores, nb_scaled_scores]\n",
    "labels = ['kNN (Scaled)', 'Naive Bayes (Scaled)']\n",
    "\n",
    "plt.boxplot(data, tick_labels=labels)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Boxplot (Scaled Models)')\n",
    "plt.show()\n",
    "\n",
    "# Print mean accuracies\n",
    "mean_accuracies = [np.mean(knn_scaled_scores), np.mean(nb_scaled_scores)]\n",
    "print(f'Mean accuracy of KNN (k=5) after Min-Max scaling: {mean_accuracies[0]:.4f}')\n",
    "print(f'Mean accuracy of Gaussian Naive Bayes after Min-Max scaling: {mean_accuracies[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (kNN):\n",
    "\n",
    "The mean accuracy of 0.8217 indicates a significant improvement in performance compared to the previous accuracy when the data wasn't scaled. As noted earlier, kNN‚Äôs performance relies heavily on the distance between points in the feature space. By scaling to the same range, we allow the kNN algorithm to function more effectively, resulting in the observed accuracy.\n",
    "\n",
    "Gaussian Naive Bayes (GNB):\n",
    "\n",
    "The mean accuracy of 0.8350 shows that GNB performs slightly better than kNN in this case. GNB's performance benefits from Min-Max scaling in terms of ensuring that the numerical range of the features does not lead to computational issues. However, because GNB operates under different assumptions about the distribution of data, its performance is inherently more stable regardless of scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(knn_accuracies, nb_accuracies, alternative = 'greater')\n",
    "\n",
    "# Report the result\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\\\n",
    "\n",
    "# Evaluate the hypothesis\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: KNN is statistically superior to Naive Bayes.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference between KNN and Naive Bayes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# K values to test\n",
    "k_values = [1, 5, 10, 20, 30]\n",
    "\n",
    "# Dictionaries to store accuracies\n",
    "train_accuracies_uniform = []\n",
    "test_accuracies_uniform = []\n",
    "train_accuracies_distance = []\n",
    "test_accuracies_distance = []\n",
    "\n",
    "# Loop over each k-value\n",
    "for k in k_values:\n",
    "    # Uniform weights\n",
    "    knn_uniform = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    knn_uniform.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_uniform = accuracy_score(y_train, knn_uniform.predict(X_train))\n",
    "    test_acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n",
    "    \n",
    "    train_accuracies_uniform.append(train_acc_uniform)\n",
    "    test_accuracies_uniform.append(test_acc_uniform)\n",
    "    \n",
    "    # Distance weights\n",
    "    knn_distance = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    knn_distance.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_distance = accuracy_score(y_train, knn_distance.predict(X_train))\n",
    "    test_acc_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n",
    "    \n",
    "    train_accuracies_distance.append(train_acc_distance)\n",
    "    test_accuracies_distance.append(test_acc_distance)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Uniform weights plot\n",
    "plt.plot(k_values, train_accuracies_uniform, label='Train Accuracy (Uniform)', marker='o')\n",
    "plt.plot(k_values, test_accuracies_uniform, label='Test Accuracy (Uniform)', marker='o')\n",
    "\n",
    "# Distance weights plot\n",
    "plt.plot(k_values, train_accuracies_distance, label='Train Accuracy (Distance)', marker='x')\n",
    "plt.plot(k_values, test_accuracies_distance, label='Test Accuracy (Distance)', marker='x')\n",
    "\n",
    "plt.title('Train and Test Accuracies for Different k-Values')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercicio 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbors (k) in a kNN classifier has a profound impact on its generalization ability. As k increases, the model shifts from being sensitive to individual training samples to a more generalized perspective that considers broader patterns in the data.\n",
    "\n",
    "When k is low, the model may memorize the training data, resulting in high training accuracy but poor performance on unseen data due to overfitting. As \n",
    "k increases, the model starts to average predictions over more neighbors, which can reduce the impact of noise and improve generalization, leading to higher test accuracy. This demonstrates that moderate values of k can help the model better reflect the true underlying distribution of the data.\n",
    "\n",
    "Beyond a certain k, the model can become too generalized, losing the ability to discern important patterns. This underfitting occurs because averaging predictions across too many neighbors can dilute the influence of relevant data points, decreased accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two possible difficulties for the Na√Øve Bayes model when learning from this dataset:\n",
    "\n",
    "1. Handling Continuous Features: Na√Øve Bayes models assume that features are categorical or follow a specific distribution (Gaussian, for example). However, several features in this dataset are continuous (e.g., age, trestbps, chol, thalach, oldpeak). The model may need to make assumptions for these variables, which may not hold true and could reduce its accuracy.\n",
    "\n",
    "2. Correlated Features: Na√Øve Bayes assumes that all features are independent, but in medical datasets like this one, many features are likely to be correlated. For instance, blood pressure (trestbps) and cholesterol (chol) might be related, and the presence of such correlations can violate the independence assumption, leading to suboptimal model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
