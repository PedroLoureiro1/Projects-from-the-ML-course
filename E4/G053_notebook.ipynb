{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = '/home/pedro_loureiro/Aprendizagem/Proj4_Aprendizagem/accounts.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_dummies = pd.get_dummies(df.iloc[:, :8], drop_first=True)\n",
    "df_dummies.drop_duplicates(inplace=True)\n",
    "df_dummies.dropna(inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df_dummies)\n",
    "sse = []\n",
    "k_values = range(2, 9)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=500, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    sse.append(kmeans.inertia_)  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Sum of Squared Errors (SSE) vs Number of Clusters (k)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('SSE')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SSE (Sum of Squared Errors) graph, the ideal number of clusters can be determined by the elbow method. The SSE decreases sharply from k = 2 to k = 4, indicating that dividing the data into more clusters significantly reduces the error. From k = 4 onward, the decrease in SSE begins to slow down, meaning that adding extra clusters does not provide such a substantial improvement in the fit. Therefore, based on the trade-off between the number of clusters and inertia, k = 4 appears to be the ideal number of clusters. This is because it is the point where increasing the number of clusters stops providing significant improvements in compression (or reduction of SSE), balancing model simplicity and explained variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-modes is designed specifically for categorical data. It uses Hamming distance and can handle the mode (most frequent category) in place of the mean. K-means works well with numerical data since it minimizes the sum of squared distances (Euclidean distance). Having this in mind, K-modes could be a better approach than traditional k-means since the majority of the feautures are categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar o dataset\n",
    "file_path = 'accounts.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Criar variáveis dummies (variáveis categóricas para numéricas)\n",
    "df_dummies = pd.get_dummies(data.iloc[:, :8], drop_first=True)\n",
    "\n",
    "# Remover duplicados e valores nulos\n",
    "df_dummies.drop_duplicates(inplace=True)\n",
    "df_dummies.dropna(inplace=True)\n",
    "\n",
    "# Normalizar os dados com StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_transformed = scaler.fit_transform(df_dummies)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)  # Especificar que queremos os 2 componentes principais\n",
    "pca_components = pca.fit_transform(data_transformed)\n",
    "\n",
    "# Calcular a variância explicada pelos 2 componentes principais\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "total_variance_explained = explained_variance.sum() * 100  # Em porcentagem\n",
    "\n",
    "# Imprimir a variância explicada\n",
    "print(f\"Explained variance by top 2 components: {explained_variance}\")\n",
    "print(f\"Total variance explained by the top 2 components: {total_variance_explained:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "file_path = 'accounts.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "df_dummies = pd.get_dummies(data.iloc[:, :8], drop_first=True)\n",
    "df_dummies.drop_duplicates(inplace=True)\n",
    "df_dummies.dropna(inplace=True)\n",
    "scaler = StandardScaler()\n",
    "data_transformed = scaler.fit_transform(df_dummies)\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(data_transformed)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_transformed)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=clusters, cmap='viridis', s=50)\n",
    "plt.xlabel('First Component')\n",
    "plt.ylabel('Second Component')\n",
    "plt.title('PCA Scatterplot with k=3 clusters')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant overlap between the clusters, especially in the central region of the plot. The boundary between the purple and teal clusters is not defined, indicating that the data points in these clusters are not well-separated. The yellow cluster appears to be more distinguishable from the other two clusters, but there is still some mixing in the lower areas.\n",
    "The fact that there is substantial overlap between clusters suggests that clear separation is not possible in this 2D representation. This indicates that the features (or components) captured by the first two principal components do not fully explain the variance needed to separate the clusters (22,76%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "file_path = '/home/pedro_loureiro/Aprendizagem/Proj4_Aprendizagem/accounts.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "df_dummies = pd.get_dummies(data.iloc[:, :8], drop_first=True)\n",
    "df_dummies.drop_duplicates(inplace=True)\n",
    "df_dummies.dropna(inplace=True)\n",
    "scaler = StandardScaler()\n",
    "data_transformed = scaler.fit_transform(df_dummies)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df_dummies['cluster'] = kmeans.fit_predict(data_transformed) \n",
    "data['cluster'] = df_dummies['cluster'].reindex(data.index) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data=data, x='job', hue='cluster', multiple=\"dodge\", stat='density', shrink=0.8, common_norm=False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Frequency distribution of \"job\" per Cluster')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(data=data, x='education', hue='cluster', multiple=\"dodge\", stat='density', shrink=0.8, common_norm=False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Frequency distribution of \"education\" per Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided graphs, here are the main differences between the clusters:\n",
    "\n",
    "Job Distribution Differences:\n",
    "\n",
    "Cluster 0: This cluster has a higher representation of blue-collar, management, and technician roles. There's a more evenly distributed presence of other professions, like admin., services, and retired, with no single job type dominating overwhelmingly.\n",
    "\n",
    "Cluster 1: This cluster has a noticeable presence in management and technician jobs, similar to Cluster 0, but it's also the only cluster that shows a slightly increased presence in student jobs. It seems to target a broad spectrum of job categories with no extreme outliers.\n",
    "\n",
    "Cluster 2: This cluster is heavily dominated by the retired category, showing an extreme concentration here compared to other clusters. Other job categories are underrepresented, making it clear that Cluster 2 primarily groups individuals who are retired.\n",
    "\n",
    "Education Distribution Differences:\n",
    "         \n",
    "Cluster 0: This cluster has a predominant representation of individuals with secondary education. There's also a reasonable proportion of individuals with tertiary education, and a smaller, but notable presence of those with primary education.\n",
    "\n",
    "Cluster 1: This cluster has a strong presence of individuals with secondary and tertiary education, with tertiary being relatively more common here than in Cluster 0. It suggests that this cluster might be more associated with higher educational levels.\n",
    "\n",
    "Cluster 2: There’s a higher proportion of individuals with primary education in this cluster compared to others. It also has a considerable portion of individuals with secondary education but a much lower representation of tertiary education, indicating lower educational levels overall.\n",
    "\n",
    "Having this in mind, Cluster 2 stands out for its dominance in the retired category and a focus on primary education, indicating an older demographic with lower educational attainment. Clusters 0 and 1 have more diverse job categories, with Cluster 0 leaning slightly more towards manual or technical jobs and Cluster 1 towards professional or academic roles. When it comes to education, Cluster 1 has a higher level of educational attainment, especially in tertiary education, compared to Cluster 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
