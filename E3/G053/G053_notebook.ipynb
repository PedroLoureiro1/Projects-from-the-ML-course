{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "\n",
    "file_path = '/home/pedro_loureiro/Aprendizagem/Proj3_Aprendizagem/parkinsons.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'MLP (No Activation)': MLPRegressor(hidden_layer_sizes=(10, 10), activation='identity', random_state=0, max_iter=200),\n",
    "    'MLP (ReLU Activation)': MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu', random_state=0, max_iter=200)\n",
    "}\n",
    "\n",
    "mae_results = {model: [] for model in models}\n",
    "\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_results[model_name].append(mae)\n",
    "\n",
    "mae_df = pd.DataFrame(mae_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=mae_df)\n",
    "plt.title('Test MAE of Each Model over 10 Runs')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP without activation functions behaves exactly like a Linear Regression (as we can see by analyzing the experimental results), limiting its ability to model non-linear patterns. Using activation functions in neural networks is important because it introduces non-linearity, allowing the MLP to learn more complex patterns and, consequently, perform better on non-linear tasks.\n",
    "\n",
    "If we were to use an MLP with activation functions like ReLU, we would expect a reduction in error (MAE), as the model would be better able to capture the complexity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "data = pd.read_csv('/home/pedro_loureiro/Aprendizagem/Proj3_Aprendizagem/parkinsons.csv')\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],  \n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],  \n",
    "    'batch_size': [32, 64, 128],\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=200, random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "grid_search.fit(X, y)  \n",
    "\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results['mean_test_score'] = -results['mean_test_score']  \n",
    "\n",
    "alpha_vals = results['param_alpha'].astype(float)\n",
    "learning_rate_vals = results['param_learning_rate_init'].astype(float)\n",
    "batch_size_vals = results['param_batch_size'].astype(float)\n",
    "mae_vals = results['mean_test_score']\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(alpha_vals, learning_rate_vals, batch_size_vals, c=mae_vals, cmap='viridis', marker='o')\n",
    "ax.set_xlabel('L2 penalty (Alpha)')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_zlabel('Batch Size')\n",
    "ax.set_title('MAE for each combination of hyperparameter')\n",
    "fig.colorbar(sc, ax=ax, label='MAE')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "print(\"Best combination of hyperparameter:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L2 Penalty (Alpha):\n",
    "\n",
    "Low (0.0001): Flexible but with a higher risk of overfitting.\n",
    "\n",
    "Moderate (0.001): A balance between flexibility and regularization.\n",
    "\n",
    "High (0.01): Reduces overfitting but may lead to underfitting.\n",
    "\n",
    "2. Learning Rate:\n",
    "\n",
    "Low (0.001): More precise convergence but slower.\n",
    "\n",
    "Moderate (0.01): A good balance between speed and stability.\n",
    "\n",
    "High (0.1): Fast convergence but with a risk of instability.\n",
    "\n",
    "3. Batch Size:\n",
    "\n",
    "Small (32): More frequent updates but higher variance in updates.\n",
    "\n",
    "Moderate (64): A good balance between updates and stability.\n",
    "\n",
    "Large (128): Stable convergence but with fewer updates.\n",
    "\n",
    "    Best Combination:\n",
    "    \n",
    "Alpha: 0.01; Batch Size: 32; Learning Rate: 0.1\n",
    "\n",
    "This combination offers a good balance between regularization, stability in learning, and frequent updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
